% This paper is part of the k2-paper project.
% Copyright 2015 Dan Foreman-Mackey (NYU) and the co-authors listed below.
%
%  RULES OF THE GAME
%
%  * 80 characters
%  * line breaks at the ends of sentences
%  * eqnarrys ONLY
%  * ``light curve'' not ``light-curve'' or ``lightcurve''
%  * Do not put in any comments that might get tweeted by @OverheardOnAph
%    (or maybe do put in a few....)
%  * that is all.
%

\documentclass[12pt,preprint]{aastex}

\pdfoutput=1

\usepackage{color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.5}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}
\usepackage{url}
\usepackage{amssymb,amsmath}
\usepackage{subfigure}
\usepackage{booktabs}

\usepackage{natbib}
\bibliographystyle{apj}

\newcommand{\project}[1]{\textsl{#1}} % hogg say
\newcommand{\kepler}{\project{Kepler}}
\newcommand{\KT}{\project{K2}}
\newcommand{\tess}{\project{TESS}}
\newcommand{\jwst}{\project{JWST}}
\newcommand{\terra}{\project{TERRA}}
\newcommand{\pdc}{\project{PDC}}
\newcommand{\license}{MIT License}
\newcommand{\projectname}{\project{ketu}}

\newcommand{\paper}{\textsl{Article}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}
\newcommand{\True}{\foreign{True}}
\newcommand{\Truth}{\foreign{Truth}}

\newcommand{\figref}[1]{\ref{fig:#1}}
\newcommand{\Fig}[1]{Figure~\figref{#1}}
\newcommand{\fig}[1]{\Fig{#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}
\newcommand{\Tab}[1]{Table~\ref{tab:#1}}
\newcommand{\tab}[1]{\Tab{#1}}
\newcommand{\tablabel}[1]{\label{tab:#1}}
\newcommand{\Eq}[1]{Equation~(\ref{eq:#1})}
\newcommand{\eq}[1]{\Eq{#1}}
\newcommand{\eqalt}[1]{Equation~\ref{eq:#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}
\newcommand{\Sect}[1]{Section~\ref{sect:#1}}
\newcommand{\sect}[1]{\Sect{#1}}
\newcommand{\sectalt}[1]{\ref{sect:#1}}
\newcommand{\App}[1]{Appendix~\ref{sect:#1}}
\newcommand{\app}[1]{\App{#1}}
\newcommand{\sectlabel}[1]{\label{sect:#1}}

\newcommand{\BIC}{{\ensuremath{\mathrm{BIC}}}}
\newcommand{\TIC}{{\ensuremath{\mathrm{TIC}}}}
\newcommand{\T}{\ensuremath{\mathrm{T}}}
\newcommand{\dd}{\ensuremath{\,\mathrm{d}}}
\newcommand{\bvec}[1]{{\ensuremath{\boldsymbol{#1}}}}
\newcommand{\appropto}{\mathrel{\vcenter{
  \offinterlineskip\halign{\hfil$##$\cr
    \propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}
\newcommand{\densityunit}{{\ensuremath{\mathrm{nat}^{-2}}}}

% TO DOS
\newcommand{\todo}[3]{{\color{#2} \emph{#1} TODO: #3}}
\newcommand{\dfmtodo}[1]{\todo{DFM}{red}{#1}}
\newcommand{\hoggtodo}[1]{\todo{HOGG}{blue}{#1}}

% Notation for this paper.
\newcommand{\flux}{{\ensuremath{f}}}
\newcommand{\ferr}{{\ensuremath{\sigma_\flux}}}
\newcommand{\attime}{{\ensuremath{t}}}
\newcommand{\basis}{{\bvec{A}}}
\newcommand{\weights}{{\bvec{w}}}

\newcommand{\period}{{\ensuremath{P}}}
\newcommand{\phase}{{\ensuremath{T^0}}}
\newcommand{\duration}{{\ensuremath{D}}}
\newcommand{\depth}{{\ensuremath{Z}}}
\newcommand{\transittime}{{\ensuremath{T}}}

\begin{document}

\title{%
    A systematic search for transiting planets in the \KT\ data % hogg changed title!
}

\newcommand{\nyu}{2}
\newcommand{\caltech}{3}
\newcommand{\cfa}{4}
\newcommand{\mpia}{5}
\newcommand{\cds}{6}
\newcommand{\princeton}{7}
\newcommand{\mpis}{8}
\author{%
    Daniel~Foreman-Mackey\altaffilmark{1,\nyu},
    Benjamin~T.~Montet\altaffilmark{\caltech,\cfa},
    David~W.~Hogg\altaffilmark{\nyu,\mpia,\cds},
    Timothy~D.~Morton\altaffilmark{\princeton},
    Dun~Wang\altaffilmark{\nyu},
    Bernhard~Sch\"olkopf\altaffilmark{\mpis},
    \etal
}
\altaffiltext{1}         {To whom correspondence should be addressed:
                          \url{danfm@nyu.edu}}
\altaffiltext{\nyu}      {Center for Cosmology and Particle Physics,
                          Department of Physics, New York University,
                          4 Washington Place, New York, NY, 10003, USA}
\altaffiltext{\caltech}  {California Institute of Technology, Pasadena, CA,
                          91125, USA}
\altaffiltext{\cfa}      {Harvard-Smithsonian Center for Astrophysics,
                          Cambridge, MA 02138, USA}
\altaffiltext{\mpia}     {Max-Planck-Institut f\"ur Astronomie,
                          K\"onigstuhl 17, D-69117 Heidelberg, Germany}
\altaffiltext{\cds}      {Center for Data Science,
                          New York University,
                          726 Broadway, 7th Floor, New York, NY, 10003, USA}
\altaffiltext{\princeton}{Department of Astrophysics, Princeton University,
                          Princeton, NJ, 08544, USA}
\altaffiltext{\mpis}     {Max Planck Institute for Intelligent Systems
                          Spemannstrasse 38, 72076 T\"ubingen, Germany}

\begin{abstract}

The photometry from the \KT\ extension of NASA's \kepler\ mission is
afflicted by systematic effects caused by the relative imprecision of the
telescope pointing and other spacecraft issues.
We present a method for searching these light curves for evidence of
exoplanets by simultaneously fitting for these systematics and the
transit signals of interest.
This method is more computationally expensive than standard search algorithms
but we demonstrate that it can be efficiently implemented and used to robustly
discover transit signals.
We apply this method to the full Campaign 1 dataset and report a list of XXX
planet candidates transiting YYY stars.
For all planet candidates, we present posterior distributions on the
properties of each system based strictly on the transit observables.
We also present a list of ZZZ candidate eclipsing binary systems.

\end{abstract}

\keywords{%
methods: data analysis
---
methods: statistical
---
catalogs
---
planetary systems
---
stars: statistics
}

\section{Introduction}

The \kepler\ Mission (cite something) has been incredibly successful at
finding transiting exoplanets in the light curves of stars (cite something).
The Mission has demonstrated that it is possible to routinely measure signals
in stellar light curves at the part-in-$10^5$ level.
Results from the primary mission include the detection of planet transits with
depths as small as 12 parts per million \citep{Barclay:2013}.

The noise floor for \kepler\ data is often quoted as 15 parts per million per
six hours of observations \citep{Gilliland:2011}.
Although they generally do not influence the ability to search for transiting
planets, larger systematic effects exist on different timescales.
One of the most serious of these is spacecraft pointing: If the detector
flat-field is not known \emph{exquisitely}, then tiny changes to the relative
illumination of pixels caused by a star's motion in the focal plane will lead
to changes in the measured or inferred brightness of the star.

The pointing of \kepler\ was stable at the (LEVEL) level (citation) throughout
the primary mission.
This stability came to an end with the failure of a critical reaction wheel.
The \KT\ Mission \citep{Howell:2014} is a follow-on to the primary \kepler\
Mission, observing about a dozen fields near the ecliptic plane, each for
$\sim 75$ days at a time.
Because of the degraded spacecraft orientation systems, the new \KT\ data
exhibit far greater pointing variations---and substantially more
pointing-induced variations in photometry---than the original \kepler\ data.
This makes good data-analysis techniques even more valuable.

Good photometry relies on either a near-perfect flat-field
and pointing model or else data-analysis techniques that are
insensitive to such variations.
The flat-field for \kepler\ was measured on the ground before the launch of
the spacecraft, but is not nearly as accurate as required to make
pointing-insensitive photometric measurements at the relevant level of precision.
In principle direct inference of the flat-field might be possible;
however, because point sources are observed with relatively limited
spacecraft motion, and only a few percent of the data are actually stored and
downloaded to Earth, there isn't enough information in the data to derive or
infer a highly accurate flat-field map.
Therefore, work on \KT\ is sensibly focused on building data-analysis
techniques that are pointing-insensitive.

Previous projects have developed methods to work with \KT\ data.
\citet{Vanderburg:2014} extract aperture photometry from the pixel data
and decorrelate with image centroid position, producing light curves for each
star that are ``corrected'' for the spacecraft motion.
These data have produced the first confirmed planet found with
\KT\ \citep{Vanderburg:2015}.
Both \citet{Aigrain:2015} and \citet{Crossfield:2015} use a Gaussian Process
model for the measured flux, with pointing measurements as the inputs, and
then ``de-trend'' using the mean prediction from that model.
Other data-driven approaches have been developed and applied to the \kepler\
data \citep[for example,][]{pdc1, pdc2, arc, dun} but they have yet to be
generalized to be applicable to \KT.

In all of these light-curve processing methodologies, the authors follow a
traditional procedure of ``correcting'' or ``de-trending'' the light curve to
remove systematic and stellar variability as a step that happens \emph{before}
the search for transiting planets.
Fit-and-subtract is dangerous: Small signals, such as planet transits, can be
partially absorbed into the best-fit stellar variability or systematics
models, making each individual transit event appear shallower.
In other words, the traditional methods are prone to over-fitting.
Because over-fitting will in general reduce the amplitude of true exoplanet
signals, small planets that ought to appear just above any specific
signal-to-noise or depth threshold could be missed because of the de-trending.
This becomes especially important as the amplitude of the noise increases.

The alternative to this approach is to simultaneously fit both the systematics
and the transit signals.
This method can push the detection limits to lower signal-to-noise while
robustly accounting for uncertainties about the systematic trends.
In particular, this allows us to \emph{marginalize} over choices in the noise
model and propagate any uncertainties to our confidence in the detection.
\dfmtodo{Do we need more about marginalization here?}

In this \paper\ we present a data-analysis technique for exoplanet search and
characterization that is insensitive to spacecraft-induced trends in the light
curves.
We assume that the trends in the observed light curves in each star due to
spacecraft motion are shared by each star, and run PCA to measure the dominant
modes.
We then search for planets by modeling the data as a linear combination of a
huge number of these basis vectors and a transit model.
Our method builds on the ideas of previous data-driven de-trending procedures
like Pre-search data conditioning \citep[\pdc;][]{pdc1, pdc2} but we can use a
much more flexible systematics model without worrying about over-fitting.

The methods developed within this paper are highly relevant to both \KT\ and
the upcoming \tess\ mission \citep{Ricker:2014}.
\tess\ will feature pointing precision of $\sim 3$ arcseconds (cite), similar
to the level of pointing drift with \KT.
Moreover, the typical star will be only observed for one month at a time, and
the typical transit detection will be at a similar signal-to-noise ratio as
with \KT.

Catalogs of transiting planets found in the \KT\ data will be important to
better understand the physical properties, formation, and evolution of
planetary systems.
These planets, especially when they orbit especially bright or late-type
stars, will be useful targets for ground-based and space-based follow-up, both
for current facilities and those planned in the near-future such as \jwst.
They will also deliver input data for next-generation population inferences
\citep{dfm}, especially for the population of planets around cool stars
\citep[for example][]{dressing}.

The \paper\ is organized as follows.
In \textsection\ref{sec:phot}, we describe our analysis of the photometric
data collected by \KT.
In \textsection\ref{sec:model}, we expain our simultaneous fit of the
systematics and transit model.
In \textsection\ref{sec:search}, we describe our search pipeline for
planet candidates.
In \textsection\ref{sec:perform}, we quantify the performace of our pipeline
through the analysis of injected planetary signals.
In \textsection\ref{sec:K2OIs}, we present the results of our search:
catalogs of \KT\ planet candidates and eclipsing binaries.
In \textsection\ref{sec:discuss}, we discuss our results and outline our
future work.


\section{Photometry}
\label{sec:phot}

The starting point for analysis is the raw pixel data.
We download the full set of XXX target pixel files for \KT's Campaign 1 from
MAST\footnote{\url{https://archive.stsci.edu/k2/}}.
Then, we extract photometry using fixed circular apertures of varying sizes
centered on the predicted location of the target star based on the world
coordinate system.
For each time series, we used circular apertures ranging in radius from 0.5 to
5 pixels (in steps of 0.5 pixels) and---following
\citet{Vanderburg:2014}---chose the aperture size that resulted in the
smallest CDPP \citep{cdpp}
with a 6 hour window.\footnote{Note that although we chose a specific
aperture, photometry for every aperture size is available online at
\dfmtodo{some URL}.}

All previous methods for analyzing \KT\ data involve some sort of
``correction'' or ``de-trending'' step based on measurements of the pointing
motion of the spacecraft \citep{Vanderburg:2014, Aigrain:2015,
Crossfield:2015}.
In our analysis, we don't do any further preprocessing of the light curves
because, as we describe in the next section, we simultaneously fit for the
trends and the transit signals that we are searching for in the raw
photometric light curves.

One key realization that is also exploited by the official \kepler\ pipeline
is that the systematic trends caused by pointing shifts and other instrumental
effects are shared---with different signs and weights---by all the stars on
the detector.
To make use of this, the \pdc\ component of the \kepler\ pipeline
removes any trends from the light curves that can be fit using a linear
combination of a small number of ``co-trending basis vectors''.
This basis of trends was found by running Principal Component Analysis (PCA)
on a large set of light curves and extracting the top few ($\sim 4$)
components \citep{pdc1, pdc2}.
Similarly, we ran PCA on the full set of Campaign 1 light curves to determine
a basis of representative trends but, unlike \pdc, we retain and use
a large number of these components.
For clarity, we will refer to our basis as a set of ``eigen light curves''
(ELCs) and the full set is made available online (SOMEURL).
\Fig{pca} shows the top ten ELCs for Campaign 1.


\section{Joint transit \& variability model}
\label{sec:model}

The key insight in our transit search method that sets it apart from the
other standard procedures is that no de-trending is necessary.
Instead, we can fit for the noise (or trends) and signal simultaneously.
This is theoretically appealing because it will be more sensitive to low
signal-to-noise transits.
The main reason for this is that the signal is not exactly orthogonal to the
systematics and any de-trending will over-fit; decreasing the amplitude of the
signal and distorting its shape.
In order to reduce this effect, most procedures use a very rigid model for
the trends.
For \KT, this has been implemented by asserting that the centroids contain
all of the information needed to describe the trends \citep{Vanderburg:2014,
Aigrain:2015, Crossfield:2015}.
In the \kepler\ pipeline, this is implemented by only allowing a small number
of PCA components to contribute to the fit in the \pdc\ procedure.
Instead, we will use a large number of ELCs---a very flexible model---and
use a physically motivated method to avoid over-fitting.

Physically, the motivation for our model---and the \pdc\ model---is that every
star on the detector should be affected by the same set of systematic effects.
These are caused by things like pointing jitter, temperature variations, and
other sources of PSF modulation and each one will be imprinted in the light
curves of many stars with varying amplitudes and signs as a result of the
varying flat field.
Therefore, while it is hard to write down a physical generative model for the
systematics, building a data-driven model might be possible.
This intuition is also exploited by other methods that model the systematics
using only empirical centroids \citep{Vanderburg:2014, Aigrain:2015,
Crossfield:2015} but our more flexible model should capture the systematic
effects more robustly.
For example, \Fig{corr} shows the application of our model---with 150
ELCs---to a light curve with no known transit signals and the precision is
excellent.
If, however, we were to apply this model blindly to a light curve with
transits, we would be at risk of over-fitting and decreasing the amplitude of
the signal.

To avoid over-fitting, in our pipeline we simultaneously fit for the transit
signal and the trends using a rigid model for the signal and a relatively
flexible model for the noise.
Specifically, we model the light curve as being generated by linear
combination of 150 ELCs and a ``box'' transit model at a specific period,
phase, and duration.
The mathematical details are given in \app{math} but in summary, since the
model is linear, we can analytically compute the likelihood
function---conditioned on a specific period, phase, and duration---for the
depth \emph{marginalized over the systematics model}.
This computation is expensive but, as described in the following sections, it
is possible to scale the method to a \KT-scale dataset.

It is worth noting that this model can be equivalently thought of as a
(computationally expensive) generalization of the ``Box Least Squares''
\citep[\project{BLS};][]{bls} method to a more sophisticated description of
the noise and systematics.
Therefore, any existing search pipeline based on \project{BLS} could, in
theory, use this model as a drop-in replacement, although some modifications
might be required for computational tractability.


\section{Search pipeline}
\label{sec:search}

In theory, search could proceed by evaluating the model described above on a
fine three-dimensional grid in period, phase, and duration.
In practice, this is computationally intractable for any grids of the
required size.
Instead, we can compute the values on this grid approximately, but at very
high precision, using a two-step procedure that is much more computationally
efficient.

The specific quantity that we will be searching is the likelihood for the
light curve of star $n$ given a set of transit parameters:
\begin{eqnarray}
p(\{\flux\}_n\,|\,\period,\,\phase,\,\duration,\,\depth) \quad.
\end{eqnarray}
We will make the simplifying assumption that each transit is independent
because it will be approximately satisfied for all but the shortest periods
and it leads to a huge computational advantage.
Under this assumption, this likelihood function can be rewritten as
\begin{eqnarray}\eqlabel{indtran}
p(\{\flux\}_n\,|\,\period,\,\phase,\,\duration,\,\depth) &\approx&
\prod_{m=1}^{M(\period,\,\phase)}
    p(\{\flux\}_n\,|\,\transittime_m(\period,\,\phase),\,\duration,\,
                    \depth)
\end{eqnarray}
where $\transittime_m(\period,\,\phase)$ is the time of the $m$-th
transit given the period $\period$ and reference time $\phase$.
\Eq{indtran} can be efficiently computed for many periods and phases if we
first compute a set of likelihood functions for single transits on a fine grid
in $\transittime_l$
\begin{eqnarray}\eqlabel{singletransit}
\left \{ p(\{\flux\}_n\,|\,\transittime_l,\,\duration_k,\,\depth)
\right\}_{l=1,\,k=1}^{L,\,K}
\end{eqnarray}
then use these results as a lookup table.

In the remainder of this section, we give more details about each step of the
search procedure but in summary, it breaks into three main steps: linear
search, periodic search, and vetting.
In the {\bf linear search} step, we evaluate the likelihood function in
\eq{singletransit} on a two-dimensional grid, coarse in transit duration
$\duration_k$ and fine in transit time $\transittime_m$.
Then in the {\bf periodic search} step, we use this two-dimensional grid to
approximately evaluate the likelihood (\eqalt{indtran}) for a
three-dimensional grid of periodic signals.
Then, we run a peak detection algorithm on this grid that discards peaks that
share transits with a shorter period candidate and signals with substantially
varying transit depths.
These transit candidates are then passed along for machine and human {\bf
vetting}.


\paragraph{Linear search}

The linear search involves hypothesizing a transit signal on a
two-dimensional grid in transit time and duration.
For each point in the grid, we use the model described in the previous
section to evaluate \emph{the likelihood function} of the transit depth.
Because the model is linear, the likelihood function for the depth
(marginalized over the model of the systematics) is a Gaussian with
analytic amplitude $L$, mean $\bar{\depth}$, and variance
$\delta\bar{\depth}^2$ derived in \app{math}.
Therefore in the linear search, we save these three numbers for a
two-dimensional grid in transit time \transittime\ and duration \duration.
The transit time grid spans the duration of Campaign 1 with half hour spacing
and we choose to only test three durations: 1.2, 2.4, and 4.8 hours.
\Fig{linear} shows the mean transit depth $\bar{\depth}$ as a function of
transit time \transittime\ for a light curve with an injected transit signal
with transits at the times indicated by the vertical green bars.
The computational complexity of this step is $\mathcal{O}(M\,K^3)$ where $M$
is the number of points in the grid and $K$ is the number of data points.


\paragraph{Periodic search}

In the period search step, we use the lookup table generated by the linear
search to compute the likelihood from \eq{indtran} on a three dimensional
grid in period \period, reference time \phase, and duration \duration.
At each point, we compute the likelihood of a model where the transit depth
varies between transits and the ``correct'' simpler model where the transit
depth is constant.
The variable depth likelihood is simply the product of amplitudes from the
initial search
\begin{eqnarray}
p_\mathrm{var}(\{\flux\}_n\,|\,\period,\,\phase,\,\duration) &=&
\prod_{m=1}^{M(\period,\,\phase)} L_m \quad,
\end{eqnarray}
and the constant depth model is
\begin{eqnarray}
p_\mathrm{const}(\{\flux\}_n\,|\,\period,\,\phase,\,\duration) &=&
\prod_{m=1}^{M(\period,\,\phase)}
    \frac{L_m}{\sqrt{2\,\pi\,{\delta\bar{\depth}_m}^2}}\,\exp \left(
        -\frac{[\depth - \bar{\depth}_m]^2}{2\,{\delta\bar{\depth}_m}^2}
    \right)
\end{eqnarray}
where the maximum likelihood depth is
\begin{eqnarray}\eqlabel{periodic-depth}
\depth &=& {\sigma_\depth}^2\,\sum_{m=1}^{M(\period,\,\phase)}
    \frac{\bar{\depth}_m}{{\delta\bar{\depth}_m}^2}
\end{eqnarray}
and the uncertainty on this depth is given by
\begin{eqnarray}\eqlabel{periodic-depth-uncert}
\frac{1}{{\sigma_\depth}^2} &=& \sum_{m=1}^{M(\period,\,\phase)}
    \frac{1}{{\delta\bar{\depth}_m}^2} \quad.
\end{eqnarray}
Note that this result is \emph{marginalized} over the parameters of the
systematics model.
Therefore, this estimate of the uncertainty on the depth takes any
uncertainty that we have about the systematics into account.

In general, the variable depth model will \emph{always} get a higher
likelihood because it is more flexible.
Therefore, to compare these two models, we use the Bayesian Information
Criterion (BIC).
The traditional definition of the BIC is
\begin{eqnarray}
-\frac{1}{2}\,\BIC_\cdot &=&
    \ln p_\cdot(\{\flux\}_n\,|\,\period,\,\phase,\,\duration)
    - \frac{J}{2} \ln N
\end{eqnarray}
where the likelihood function is maximized, $J$ is an estimate of the model
complexity and $N$ is the effective sample size.
To emphasize that $J$ and $N$ are tuning parameters of the method, we
redefine the BIC as the TIC
\begin{eqnarray}
\TIC_\cdot &=&
    \ln p_\cdot(\{\flux\}_n\,|\,\period,\,\phase,\,\duration) - \alpha
\end{eqnarray}
and we choose $\alpha$ heuristically.
For the K2 Campaign 1 dataset, we find that $\alpha \sim 1250$ leads to robust
recovery of injected signals while still being fairly insensitive to false
signals.

To limit memory consumption, in the periodic search, we actually profile (or
maximize) over \phase\ and \duration\ subject to the constraint that
$\TIC_\mathrm{const} > \TIC_\mathrm{var}$.
This results in a one-dimensional ``periodogram'' measuring the quality of
the fit as a function of period.
To choose the ``best'' candidate signal, we found that the best quantity to
select on was the signal-to-noise with which the depth was measured at a
given period, $\depth/\sigma_\depth$, computed using
Equations~(\ref{eq:periodic-depth}) and (\ref{eq:periodic-depth-uncert}).
Figure SOMEFIGURE shows the signal-to-noise of the depth measurement as a
function of period for SOMECANDIDATE.

After selecting the best candidate based on the signal-to-noise of the depth,
we mask out the sections of the linear search corresponding to these transits
and iterate the periodic search.
Under our assumption of independent transits, this is equivalent to removing
the sections of data that have a transit caused by the highest peak.
For the purposes of this \paper, we iterate the periodic search until we find
three peaks for each light curve.
This will necessarily miss the smallest and longest period planets in systems
with more than three transiting planets but given the precision---or lack
thereof---of the current method, we deemed this number sufficient.


\paragraph{Initial candidate list}

The periodic search procedure returned three signals per target so this gave
an initial list of ZZZZ candidates.
The vast majority of these signals are, of course, not induced by a
transiting planet.
Therefore to reduce the search space, we estimate the signal-to-noise of each
candidate by comparing the peak height to a robust estimate of variance in
\TIC\ values across period.
This is not the same criterion used to select the initial three peaks but we
found that at this stage, it produced a more complete and pure sample.
A cut in this quantity can reject most variable stars and low signal-to-noise
candidates that can't be reliably recovered from the data.
Figure SOMEFIGURE shows the distribution of signal-to-noise for a set of
injections and compares it to the distribution of false candidates that are
proposed by the pipeline.
To minimize contamination from false alarms but maximize our sensitivity, we
choose a threshold of 15 \dfmtodo{right number?}.
We also find that the signals with periods $\lesssim 4$ days are very
contaminated by false alarms.
This might be because of the fact that our independence assumption
(SOMEEQUATION) breaks down at these short periods.
Therefore, we discard all signals with periods shorter than 4 days.
After these cuts, we're left with 700 \dfmtodo{right number?} candidates that
we then examine by hand.
The full list of peaks and their relevant meta data is available online at
\dfmtodo{SOMEURL}.


\paragraph{Hand vetting}

After our initial cuts on the candidate list, the majority of signals are
still false alarms mostly due to variable stars or single outlying data
points.
It should be possible to construct a more robust machine vetting algorithm
that discards these samples without missing real transits (CITE SOME THINGS)
but for the purposes of this \paper, we simply inspect the light curve for
each of the YYYY candidates \emph{by hand} to discard signals that are not
convincing transits.
The results of this vetting can be seen online at SOMEURL.

This vetting resulted in a list of ZZZZ transiting candidates.
Many of these signals are due to astrophysical ``false positives'' like
eclipsing binaries or blending.
We address this effect in the following section and separate the list of
candidates into a list of astrophysical false positives and planet candidates.


\paragraph{Astrophysical false positives}

A major problem with any transit search is the potential confusion between
transiting planets and eclipsing binaries.
Ground-based surveys before \kepler\ often had a false-positive rate
of 30-40\%. (because of shit?)
In the primary \kepler\ mission, this number was significantly lower:
the false positive rate was only 5-10\% \citep{Morton:2011}.
Such improvement was enabled by the relatively small pixels, stable
pointing, and high precision of \kepler.
In \kepler\ data, false positives can be reliably separated from
planet candidates by looking for differences in the position of the
centroid of light during transit events with respect to the
out-of-transit data (e.g. citation), variations in the depth of
alternating transits (e.g. citation), and evidence for ellipsoidal
variations between transits (e.g. citation).

In \KT, the precision of these tests is lower and they must be applied with
care.
There are typically only a handful of transits, meaning differences between
``odd'' and ``even'' transits must be large to create a significant
difference.
Searching for ellipsoidal varaitions is hindered by the short time baseline
and the increased photometric uncertainty in \KT\ data.
Centroid variations are feasible in \KT\ but must be treated differently than
in the original \kepler\ mission where this effect was generally measured
using difference imaging (cite Howell).

To obtain an initial assessment of the astrophysical false positive rate in
our catalog of planet candidates, we test for centroid offsets and variable
depth transits using the machinery that we have already established for
modeling the systematic trends in the data.
This is only the first word and a more complete characterization of our
catalog's reliability is forthcoming (Montet, Foreman-Mackey, \etal\ in prep).

To measure \emph{centroid offsets}, we start by empirically measuring the
pixel centroid time series for each candidate by modeling the pixels near the
peak as a two-dimensional quadratic and finding the maximum at each time.
This method has been shown to produce higher precision centroid measurements
than center-of-light estimates (@HOGG: is this true? Is there a citation? MJ
in prep?).
Figure SOMEFIGURE shows the measure $x$ and $y$ pixel coordinate traces for
SOMECANDIDATE.
Much like the photometry, this signal is dominated by the rigid body motion
of the spacecraft and we can, in fact, model it identically.
Previously (SOMEEQUATION), we modeled the light curve as a linear combination
of ELCs and a simple box transit model at a given period, phase, and duration.
Under this model, the maximum likelihood depth can be computed analytically.
If we apply \emph{exactly the same model} to the centroid trace, the ``depth''
that we compute becomes the centroid motion in transit in units of pixels.
Since the motions won't necessarily point in a consistent direction across
transits, we treat each transit independently and report the average offset
amplitude weighted by the precision of each measurement.
To compute the significance of a centroid offset, we bootstrap the offset
amplitude for models at the same period and duration but randomly oriented
phases.
If the centroid measured for the candidate transit is substantially larger
(by SOMEAMOUNT) than the random realizations, then the candidate is labeled
as a false positive.
Figure SOMEFIGURE shows the distribution of centroid offsets across our
catalog and YYYY---or ZZZZ\%---of our candidates fail this test.

We also test the consistency \emph{between even and odd transit depths} for
each of our candidates.
As described in SOMESECTION, our method for transit search automatically
rejects signals with wildly varying depths but.

Blah blah blah.
Therefore, false positives caused by background eclipsing binaries which could
be identified as such by \kepler\ data alone will be much more difficult to
distinguish in \KT\ data, making high-contrast adaptive optics images of
planet candidates an essential follow-up tool.


\section{Performance}

To test the performance of our method, we conducted a suite of XXXX injection
and recovery tests.
For each test, we inject the signal from a realistic planetary system into the
raw aperture photometry and passed it through the full pipeline.
These synthetic systems have periods and radius ratios drawn uniformly in
\dfmtodo{some range} and uniformly distributed impact parameters and phases.
The eccentricities are drawn from a beta distribution with \dfmtodo{some
parameters; cite Kipping} and the multiplicity is set to match the observed
multiplicity distribution in the KOI catalog \dfmtodo{cite}.
MA limb darkening, exposure time integration, etc.

After running these light curves with injections through the full pipeline, if
the resulting period and phase of the recovered signal were within YYYY of the
true injected values, we deemed the injection a success.
\Fig{completeness} shows the fraction of recovered signals as a function of the
physical parameters of the injection and the magnitude of the star.
As expected, the shallower transits at longer periods are recovered less
robustly.



\section{Results}

After running the 21XXX light curves through our full pipeline, we end up
with XXXX TCEs \dfmtodo{change this name} and after hand vetting, we are left
with YYYY convincing transiting candidates.
Of these candidates, ZZZ are probable binaries with obviously visible
secondary eclipses and we list these candidates in Table \dfmtodo{SOMETABLE}.
This leaves WWW planet candidates that are listed in Table
\dfmtodo{SOMETABLE}.

\paragraph{Posterior constraints on physical parameters}

To fit for the observable parameters of the transit signals in detail, we use
the same linear model for the systematics but then we combine this with a
physical transit model (M\&A, etc.).
Since this transit model is no longer linear, we use Markov Chain Monte Carlo
(MCMC; cite emcee) to draw posterior samples from this model.
At each step in the chain, we \emph{marginalize over the model of the
systematics}.
This means that we don't de-trend the data or make any hard decision about
the actual value of the systematics.
We sample the stellar density, limb-darkening coefficients, period, phase,
radius ratio, impact parameter, eccentricity, and argument of periapsis.
\dfmtodo{List priors.}
The quantiles of these posterior samplings are given in table \dfmtodo{some
table} and the full chains are available electronically \dfmtodo{some URL}.


\section{Discussion}

We have searched the \KT\ Campaign~1 data set for exoplanet transit signals.
Our search is novel because it includes a very flexible systematics model,
which is fit simultaneously with the exoplanet signals of interest, and
marginalized out.
By this method, we find XXX new exoplanets, which we have vetted by ZZZ
method and characterized by probabilistic modeling.
The candidates are listed in \dfmtodo{some table} and
posterior distributions of planet candidate properties are available at
\dfmtodo{thisurl}.

The systematics model is a 150-parameter linear combination of PCA components
derived from the full set of 20,000 stellar lightcurves.
That is, it presumes that the systematics afflicting each star are shared in
some way across all other stars.
It is our belief---although not a strict assumption of our model---that these
systematics are caused primarily by pointing drifts, or movements of the
pixels in the focal plane relative to the stars.
In principle, if the systematics \emph{are} dominated by pointing issues, the
systematics model could require only three parameters---three Euler
angles---not 150 amplitudes.
However, because (as the pointing drifts) each star sees it's own unique
local patch of flat-field variations, the mapping from pointing drifts to
brightness variations can be extremely non-linear.
Furthermore, because when the pointing is moving fast, there is effectively
a smearing of the point-spread function, there are effects keyed to the
time derivative of the Euler angles as well.
The large number (150) of linear coefficients gives the linear model the
freedom to model complex non-linear behavior; we are trading off parsimony
in parameters with the enormous computational advantages of maintaining
linearity (and therefore also convexity).
The computational advantages of the linear model are three-fold:
Convexity obviates searching in parameter space for alternative modes;
linear least-squares optimization can be performed with simple linear algebra;
given Gaussian uncertainties and uninformative priors, marginalizations over
the linear parameters also reduces to pure linear algebra.

The posterior distributions we release are given in terms of observational
quantities (rather than physical quantities) and are specifically \emph{NOT}
marginalized over stellar properties.
In this work, we are agnostic about fundamental properties of the host stars.
The only assumptions we make are that the star targeted by the \KT\
team is truly the planet host, and that there is no dilution by other stars in
any aperture.
As a result, these posterior distributions reflect the maximum possible
uncertainty in parameters such as the planet radius, which depend sensitively
on properties of the host star.

The goal of this \paper\ was to get exoplanet candidates out of the \KT\ pixel-level data,
it was not to generate light curves...
the project does not really produce corrected light curves at all; why not?

This is not the last time the \KT\ data will be searched...
list some deficiencies of this search method
(these might include approximations in the PHIC,
the fact that it deals well with spacecraft systematics but not stellar variability,
and that it isn't sensitive to single transits (which must be common)).

The machine and hand vetting leave much to be desired...
list some deficiencies of the vetting.

Characterization of these candidates can be much improved...
list the things we could know if we had follow-up observations of the stars.

To use these distributions to characterize the properties of specific systems,
one should weight our samples by inferred stellar properties...
Perhaps give a specific example here with some detailed discussion or math.

The completeness and false-positive assessment could be vastly improved...
completeness estimates are not as extensive as desired;
there is essentially no estimate of the non-astrophysical false positive rate.

We have not attempted to do any kind of populations analysis...
but presumably \KT\ data will rock this.  In what ways?

\acknowledgments
It is a pleasure to thank
\ldots\
for helpful contributions to the ideas and code presented here.
DFM, DWH, and DW were partially supported by the National Science Foundation (grant IIS-1124794),
the National Aeronautics and Space Administration
(grant NNX12AI50G), and the Moore--Sloan Data Science Environment at NYU.
BTM was supported by a National Science Foundation Graduate Research
Fellowship (grant DGE‚Äê1144469).
This research made use of the NASA \project{Astrophysics Data System}.

{\it Facilities:} \facility{Kepler}

\clearpage
\appendix

\section{Mathematical model}\sectlabel{math}

Formally, this can be written for the light curve of the $k$-th star as
\begin{eqnarray}\eqlabel{linear-model}
\bvec{f}_k &=& \bvec{A}\,\bvec{w}_k + \mathrm{noise}
\end{eqnarray}
where
\begin{eqnarray}
\bvec{f}_k &=& \left (\begin{array}{cccc}
    f_{k,1} & f_{k,2} & \cdots & f_{k,N}
\end{array}\right )^\T
\end{eqnarray}
is the list aperture fluxes for star $k$ observed at $N$ times
\begin{eqnarray}
\bvec{t} &=& \left (\begin{array}{cccc}
    t_{1} & t_{2} & \cdots & t_{N}
\end{array}\right )^\T \quad.
\end{eqnarray}
In \eq{linear-model}, the design matrix is given by
\begin{eqnarray}
\bvec{A} &=& \left (\begin{array}{cccccc}
    x_{1,1} & x_{2,1} & \cdots & x_{J,1} & 1 & m_\bvec{\theta}(t_1) \\
    x_{1,2} & x_{2,2} & \cdots & x_{J,2} & 1 & m_\bvec{\theta}(t_2) \\
    && \vdots &&&\\
    x_{1,N} & x_{2,N} & \cdots & x_{J,N} & 1 & m_\bvec{\theta}(t_N)
\end{array}\right )
\end{eqnarray}
where the $x_{j,n}$ are the basis ELCs---with the index $j$ running over
components and the index $n$ running over time---and $m_\bvec{\theta}(t)$ is
the transit model
\begin{eqnarray}
m_\bvec{\theta}(t) &=& \left\{\begin{array}{cl}
-1 & \mathrm{if\,}t\,\mathrm{in\,transit} \\
0 & \mathrm{otherwise}
\end{array}\right.
\end{eqnarray}
parameterized by a period, phase, and transit duration (these parameters are
denoted by \bvec{\theta}).

Assuming that the uncertainties on $\bvec{f}_k$ are Gaussian and constant,
the maximum likelihood solution for \bvec{w} is
\begin{eqnarray}
{\bvec{w}_k}^* &\gets& \left( \bvec{A}^\T\,\bvec{A} \right)^{-1}\,
                       \bvec{A}^\T\,\bvec{f}_k
\end{eqnarray}
and the marginalized likelihood function for the transit depth is a Gaussian
with the mean given by the last element of ${\bvec{w}_k}^*$ and the variance
given by the lower-right element of the matrix
\begin{eqnarray}
{\bvec{\delta w}_k}^2 &\gets& {\sigma_k}^2 \,
            \left( \bvec{A}^\T\,\bvec{A} \right)^{-1}
\end{eqnarray}
where $\sigma_k$ is the uncertainty on $\bvec{f}_k$.
The amplitude of this Gaussian is given by
\begin{eqnarray}\eqlabel{depth-likelihood}
\mathcal{L}_k &=& \frac{1}{(2\,\pi\,{\sigma_k}^2)^{N/2}}\,\exp\left(
-\frac{1}{2\,{\sigma_k}^2}\,
\left| \bvec{f}_k - \bvec{A}\,{\bvec{w}_k}^* \right|^2
\right)
\end{eqnarray}
evaluated at the maximum likelihood value ${\bvec{w}_k}^*$.

Remember that the likelihood defined in \eq{depth-likelihood} is a function of
the parameters \bvec{\theta} (the period, phase, and duration of the planet).
In principle, this means that a transit search can proceed by building a dense
3D grid of \bvec{\theta} values and evaluating the likelihood of the data
$\mathcal{L}_k$ using \eq{depth-likelihood} at each point in the grid.

\clearpage
\bibliography{k2}
\clearpage


\begin{figure}[p]
\begin{center}
\includegraphics{figures/pca.pdf}
\end{center}
\caption{%
The top 10 eigen light curves (ELCs) generated by running principal component
analysis on all the aperture photometry from Campaign 1.
\figlabel{pca}}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics{figures/corr.pdf}
\end{center}
\caption{%
A demonstration of the ELC fit to the aperture photometry for EPIC 201374602.
\emph{Top:} The black points show the aperture photometry and the green line
is the maximum likelihood linear combination of ELCs.
The estimated 6-hour precision of the raw photometry is 264 ppm.
\emph{Bottom:} The points show the residuals of the data away from the ELC
prediction.
The 6-hour precision of this light curve is 31 ppm.
\figlabel{corr}}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics{figures/linear.pdf}
\end{center}
\caption{%
The estimated transit depth as a function of transit time as computed in a
linear search.
This light curve has an injected signal with transits at the times indicated
with green vertical lines.
\figlabel{linear}}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics{figures/periodic.pdf}
\end{center}
\caption{%
The periodogram for the same signal as in \fig{linear}.
This plot was generated by the periodic search procedure.
The true period is indicated by the vertical green line.
\figlabel{periodic}}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics{figures/depth_s2n.pdf}
\end{center}
\caption{%
The signal-to-noise of the transit depth as a function of period for the same
signal as in \fig{periodic}.
The correct injected period is indicated by the vertical green line.
\figlabel{depth-s2n}}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics[width=\textwidth]{figures/completeness.pdf}
\end{center}
\caption{%
\figlabel{completeness}}
\end{figure}

\begin{table}[p]
\begin{center}
\small
\include{tab-cand}
\end{center}
\caption{%
Candidates
\tablabel{cand}}
\end{table}

\begin{table}[p]
\begin{center}
\include{tab-eb}
\end{center}
\caption{%
Probable binaries.
\tablabel{eb}}
\end{table}

\end{document}
