%
%  RULES OF THE GAME
%
%  * 80 characters
%  * line breaks at the ends of sentences
%  * eqnarrys ONLY
%  * ``light curve'' not ``light-curve'' or ``lightcurve''
%  * that is all.
%

\documentclass[12pt,preprint]{aastex}

\pdfoutput=1

\usepackage{color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.5}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}
\usepackage{url}
\usepackage{amssymb,amsmath}
\usepackage{subfigure}

\newcommand{\project}[1]{\textsl{#1}} % hogg say
\newcommand{\kepler}{\project{Kepler}}
\newcommand{\KT}{\project{K2}}
\newcommand{\tess}{\project{TESS}}
\newcommand{\terra}{\project{TERRA}}
\newcommand{\license}{MIT License}
\newcommand{\projectname}{\project{ketu}}

\newcommand{\paper}{\textsl{Article}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}
\newcommand{\True}{\foreign{True}}
\newcommand{\Truth}{\foreign{Truth}}

\newcommand{\figref}[1]{\ref{fig:#1}}
\newcommand{\Fig}[1]{Figure~\figref{#1}}
\newcommand{\fig}[1]{\Fig{#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}
\newcommand{\Tab}[1]{Table~\ref{tab:#1}}
\newcommand{\tab}[1]{\Tab{#1}}
\newcommand{\tablabel}[1]{\label{tab:#1}}
\newcommand{\Eq}[1]{Equation~(\ref{eq:#1})}
\newcommand{\eq}[1]{\Eq{#1}}
\newcommand{\eqalt}[1]{Equation~\ref{eq:#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}
\newcommand{\Sect}[1]{Section~\ref{sect:#1}}
\newcommand{\sect}[1]{\Sect{#1}}
\newcommand{\sectalt}[1]{\ref{sect:#1}}
\newcommand{\App}[1]{Appendix~\ref{sect:#1}}
\newcommand{\app}[1]{\App{#1}}
\newcommand{\sectlabel}[1]{\label{sect:#1}}

\newcommand{\BIC}{{\ensuremath{\mathrm{BIC}}}}
\newcommand{\TIC}{{\ensuremath{\mathrm{TIC}}}}
\newcommand{\T}{\ensuremath{\mathrm{T}}}
\newcommand{\dd}{\ensuremath{\,\mathrm{d}}}
\newcommand{\bvec}[1]{{\ensuremath{\boldsymbol{#1}}}}
\newcommand{\appropto}{\mathrel{\vcenter{
  \offinterlineskip\halign{\hfil$##$\cr
    \propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}
\newcommand{\densityunit}{{\ensuremath{\mathrm{nat}^{-2}}}}

% TO DOS
\newcommand{\todo}[3]{{\color{#2} \emph{#1} TODO: #3}}
\newcommand{\dfmtodo}[1]{\todo{DFM}{red}{#1}}
\newcommand{\hoggtodo}[1]{\todo{HOGG}{blue}{#1}}

% Notation for this paper.
\newcommand{\flux}{{\ensuremath{f}}}
\newcommand{\ferr}{{\ensuremath{\sigma_\flux}}}
\newcommand{\attime}{{\ensuremath{t}}}
\newcommand{\basis}{{\bvec{A}}}
\newcommand{\weights}{{\bvec{w}}}

\newcommand{\period}{{\ensuremath{P}}}
\newcommand{\phase}{{\ensuremath{T^0}}}
\newcommand{\duration}{{\ensuremath{D}}}
\newcommand{\depth}{{\ensuremath{Z}}}
\newcommand{\transittime}{{\ensuremath{T}}}

\begin{document}

\title{%
    A systematic search for transiting planets using \KT
}

\newcommand{\nyu}{2}
\newcommand{\caltech}{3}
\newcommand{\cfa}{4}
\newcommand{\mpia}{5}
\newcommand{\cds}{6}
\newcommand{\mpis}{7}
\author{%
    Daniel~Foreman-Mackey\altaffilmark{1,\nyu},
    Benjamin~T.~Montet\altaffilmark{\caltech,\cfa},
    David~W.~Hogg\altaffilmark{\nyu,\mpia,\cds},
    Bernhard~Sch\"olkopf\altaffilmark{\mpis},
    Dun~Wang\altaffilmark{\nyu},
    \etal
}
\altaffiltext{1}         {To whom correspondence should be addressed:
                          \url{danfm@nyu.edu}}
\altaffiltext{\nyu}      {Center for Cosmology and Particle Physics,
                          Department of Physics, New York University,
                          4 Washington Place, New York, NY, 10003, USA}
\altaffiltext{\caltech}  {California Institute of Technology, Pasadena, CA,
                          91125, USA}
\altaffiltext{\cfa}      {Harvard-Smithsonian Center for Astrophysics,
                          Cambridge, MA 02138, USA}
\altaffiltext{\mpia}     {Max-Planck-Institut f\"ur Astronomie,
                          K\"onigstuhl 17, D-69117 Heidelberg, Germany}
\altaffiltext{\cds}      {Center for Data Science,
                          New York University,
                          726 Broadway, 7th Floor, New York, NY, 10003, USA}
\altaffiltext{\mpis}     {Max Planck Institute for Intelligent Systems
                          Spemannstrasse 38, 72076 T\"ubingen, Germany}

\begin{abstract}

The photometry from the \KT\ extension of NASA's \kepler\ mission is
afflicted by systematic effects caused by the relative imprecision of the
telescope pointing, and possibly other spacecraft effects.
We present a method for searching these light curves for evidence of
exoplanets by simultaneously fitting for these systematics and the
transit signals of interest.
This method is more computationally expensive than standard search algorithms
but we demonstrate that it can be efficiently implemented and used to
discover transit signals in the existing dataset.
We apply this method to the full Campaign 1 dataset and report a list of XXX
planet candidates transiting YYY stars.
We also present a list of probably astrophysical false positives, and
the results of some follow-up spectroscopic data on a few of the
planet candidates
The most interesting systems are JJJ and HHH.

\end{abstract}

\keywords{%
methods: data analysis
---
methods: statistical
---
catalogs
---
planetary systems
---
stars: statistics
}

\section{Introduction}



The \kepler\ Mission (cite something) has been incredibly succesful at
finding periodic transits (eclipses) in the light curves (time-series
photometry) of stars (cite something).
The Mission has demonstrated that it is possible to routinely measure
signals in stellar light curves at the part-in-$10^5$ level.
Results from the primary mission include the detection of planet
transits with depths as small as (XX) parts per million (cite Barclay).

The noise floor for \kepler\ data is often quoted as 15 parts per million per
six hours of observations (cite someone- Gilliland 11?). 
Although they generally do not influence the ability to search for transiting
planets, larger systematic (noise shits) exist on different timescales. 
One of the most serious of these is spacecraft pointing:
If the detector flat-field is not known \emph{exquisitely}, then
tiny changes to the relative illumination of pixels caused by a star's
motion in the focal plane (or caused by motion of the focal-plane
boresight or orientation relative to the celestial sphere) will lead
to changes in the measured or inferred brightness of the star.

The pointing of \kepler\ was stable at the (LEVEL) level (citation) throughout
the primary mission. 
This primary mission came to an end with the failure of a critical
reaction wheel, rendering the spacecraft less able to maintain precise
pointing.
The \KT\ Mission (cite something-Howell 14) is a follow-on to the primary
\kepler\ Mission, observing approximately a dozen fields near the ecliptic
plane, each for 75 days at a time.
Because of the degraded spacecraft orientation systems, the new
\KT\ data will in general exhibit far greater pointing
variations---and far greater pointing-induced variations in
photometry---than the original \kepler\ data.
This makes good data-analysis techniques extremely valuable.

Good photometry relies on either a near-perfect flat-field
and pointing model or data-analysis techniques that are
insensitive to such variations.
The flat-field for \kepler\ was measured on the ground before the launch of 
the spacecraft, but (blah blah it sucks for describing the FF now).
Inference of the flat field is challenging because of the nature of the
spacecraft's observations: point sources are observed with relatively limited
spacecraft motion, and only a few percent of the data are actually stored and
downloaded to Earth.
Therefore, much of the effort in \KT\ 

In this \paper\ we present an example of the latter---a
data-analysis technique for exoplanet search and characterization that
is insensitive to spacecraft-induced (or, in general,
hardware-induced) trends in the light curves.




...simultaneous fitting of systematics and transit signals is a way of
searching for (and measuring) exoplanet signals independent of the
systematics, provided that the systematics lie in the domain of the
model.

...put in a warning here about \foreign{a priori} ``fit and subtract''
or ``correction'' of light curves.  These methods are very sensitive
to over-fitting in general, so either the investigator must
artificially restrict the model freedom or else do clever
data-censoring methods to avoid it (cite Dun AAS and in prep here).

...DFM: DOES THIS METHOD MARGINALIZE OUT THE SYSTEMATICS?  (It could,
right, because the mean and variance of the Gaussian likelihood are
analytic (though maybe expensive).)  If we \emph{do} marginalize (or
maybe either way), we need to emphasize here the difference between
simultaneously \emph{estimating} the signal and the noise as opposed
to simultaneously \emph{fitting} the signal and the noise and
marginalization out of the latter.  Key idea: Don't come to a
conclusion about (estimate) anything you don't care about, if you can
possibly avoid it!

...we are not the first to search the \KT\ data (cite Crossfield, etc),
and we build on ideas from \kepler\ searches (cite Petigura and
\kepler).

...our goals are to find new exoplanets, to generate targets for
ground-based and space-based (JWST, even) follow-up, and to understand
the full population of exoplanets.  We (as a community) have made
progress on all these fronts, but there is much more to do.

...The future is \tess\ (cite something); everything we do here is
highly relevant to that important mission!

\section{Photometry}

The starting point for analysis is the raw pixel data.
We download the full set of XXX target pixel files for \KT's Campaign 1 from
MAST\footnote{\url{https://archive.stsci.edu/k2/}}.
Then, we extract photometry using fixed circular apertures of varying sizes
centered on the mean position of the brightest star (on average) in each
frame.
The centroids and initial flux estimates were measured using
\project{simplexy}, a component of the \project{Astrometry.net} pipeline
\citep{astrometry}.
For each time series, we used 20 circular apertures ranging in radius from 0.5
to 10 pixels and---following \citet{vanderberg-a}---chose the aperture size
that resulted in the smallest CDPP \dfmtodo{CITE CDPP-Jessie Christiansen,
Jon Jenkins, et al 2012} with a 6 hour
window.\footnote{Note that although we chose a specific aperture, photometry
for every aperture size is available online at \dfmtodo{some URL}.}

Most methods for analyzing \KT\ data---and \kepler\ data, for that
matter---involves some sort of pre-processing or ``de-trending'' step.
For example, \citet{vanderberg-a} measure the centroid motion for each star
and regress out any signal in their aperture photometry that can be fit by
this motion.
Similarly, \citet{crossfield} iteratively construct a robust Gaussian Process
model for the photometry as a function of the measured centroids and de-trend
using the mean prediction from that model.
In our analysis, we don't do any further preprocessing of the light curves
because, as we describe in the next section, we simultaneously fit for the
trends and the transit signals that we are searching for.

One key realization that is also exploited by the official \kepler\ pipeline
is that the systematic trends caused by pointing shifts and other instrumental
effects are shared---with different signs and weights---by all the stars on
the detector.
To exploit this fact, the \project{PDC} component of the \kepler\ pipeline
removes any trends from the light curves that can be fit using a linear
combination of a small number of ``eigen-light curves'' (ELCs) found by
running Principal Component Analysis (PCA) on a set of light curves
\citep{map-pdc1, map-pdc2}.
Similarly, we ran PCA (as implemented by the \project{scikit-learn} project
\citealt{sklearn}) on the full set of Campaign 1 light curves to determine a
basis of representative ELCs.
We chose to use the top 150 components---many more than are normally
used---for our model described in the following section.
\Fig{pca} shows the top few ELCs.


\section{Joint transit \& variability model}

The key insight in our transit search method that sets it apart from the
other standard procedures is that no de-trending is necessary.
Instead, we can fit for the noise (or trends) and signal simultaneously.
This is theoretically appealing because it will be more sensitive to low
signal-to-noise transits.
The main reason for this is that the signal is not exactly orthogonal to the
systematics and the de-trending will over-fit causing the signal to be
distorted and weaker.
In order to reduce this effect, most procedures use a very rigid model for
the trends.
For \KT, this has been implemented by asserting that the centroids contain
all of the information needed to describe the trends.
In the \kepler\ pipeline, this is implemented by only allowing a small number
of PCA components to contribute to the fit.

Physically, the motivation for our model---and the PDC model---is that every
star on the detector should be affected by same set of systematic effects.
These are caused by things like pointing jitter, temperature variations, and
other sources of PSF modulation and each one will be imprinted in the light
curves of many stars with varying amplitudes and signs.
Therefore, while it is hard to write down a physical generative model for the
systematics, building a data-driven model might be possible.
This intuition is also exploited by other methods that model the systematics
using only empirical centroids \citep{vanderberg-a, crossfield} but out more
flexible model should capture the systematic effects more robustly but there
is a risk of over-fitting.
\Fig{corr} shows the application of this model to a light curve with no known
transit signals.

To avoid over-fitting in our pipeline, we simultaneously fit for the transit
signal and the trends using a rigid model for the signal and a relatively
flexible model for the noise.
Specifically, we model the light curve as being generated by linear
combination of the 150 basis light curves and a ``box'' transit model at a
given period and phase.


\section{Search pipeline}

In theory, search could proceed by evaluating the model described above on a
fine three-dimensional grid in period, phase, and duration.
In practice, this is computationally intractable for any grids of the
required size.
Instead, we can compute the values on this grid approximately, but at very
high precision, using a two-step procedure that is much more computationally
efficient.

The specific quantity that we will be searching is the likelihood for the
light curve of star $n$ given a set of transit parameters:
\begin{eqnarray}
p(\{\flux\}_n\,|\,\period,\,\phase,\,\duration,\,\depth) \quad.
\end{eqnarray}
We will make the simplifying assumption that each transit is independent
because it will be approximately satisfied for all but the shortest periods
and it leads to a huge computational advantage.
Under this assumption, this likelihood function can be rewritten as
\begin{eqnarray}\eqlabel{indtran}
p(\{\flux\}_n\,|\,\period,\,\phase,\,\duration,\,\depth) &\approx&
\prod_{m=1}^{M(\period,\,\phase)}
    p(\{\flux\}_n\,|\,\transittime_m(\period,\,\phase),\,\duration,\,
                    \depth)
\end{eqnarray}
where $\transittime_m(\period,\,\phase)$ is the time of the $m$-th
transit given the period $\period$ and reference time $\phase$.
\Eq{indtran} can be efficiently computed for many periods and phases if we
first compute a set of likelihood functions for single transits on a fine grid
in $\transittime_l$
\begin{eqnarray}\eqlabel{singletransit}
\left \{ p(\{\flux\}_n\,|\,\transittime_l,\,\duration_k,\,\depth)
\right\}_{l=1,\,k=1}^{L,\,K}
\end{eqnarray}
then use these results as a lookup table.

In the remainder of this section, we give more details about each step of the
search procedure but in summary, it breaks into three main steps: linear
search, periodic search, and vetting.
In the {\bf linear search} step, we evaluate the likelihood function in
\eq{singletransit} on a two-dimensional grid, coarse in transit duration
$\duration_k$ and fine in transit time $\transittime_m$.
Then in the {\bf periodic search} step, we use this two-dimensional grid to
approximately evaluate the likelihood (\eqalt{indtran}) for a
three-dimensional grid of periodic signals.
Then, we run a peak detection algorithm on this grid that discards peaks that
share transits with a shorter period candidate and signals with substantially
varying transit depths.
These transit candidates are then passed along for machine and human {\bf
vetting}.


\paragraph{Linear search}

The linear search involves hypothesizing a transit signal on a
two-dimensional grid in transit time and duration.
For each point in the grid, we use the model described in the previous
section to evaluate \emph{the likelihood function} of the transit depth.
Because the model is linear, the likelihood function for the depth
(marginalized over the model of the systematics) is a Gaussian with
analytic amplitude $L$, mean $\bar{\depth}$, and variance
$\delta\bar{\depth}^2$ derived in \app{math}.
Therefore in the linear search, we save these three numbers for a
two-dimensional grid in transit time \transittime\ and duration \duration.
The transit time grid spans the duration of Campaign 1 with half hour spacing
and we choose to only test three durations: 1.2, 2.4, and 4.8 hours.
\Fig{linear} shows the mean transit depth $\bar{\depth}$ as a function of
transit time \transittime\ for a light curve with an injected transit signal
with transits at the times indicated by the vertical green bars.
The computational complexity of this step is $\mathcal{O}(M\,K^3)$ where $M$
is the number of points in the grid and $K$ is the number of data points.


\paragraph{Periodic search}

In the period search step, we use the lookup table generated by the linear
search to compute the likelihood from \eq{indtran} on a three dimensional
grid in period \period, reference time \phase, and duration \duration.
At each point, we compute the likelihood of a model where the transit depth
varies between transits and the ``correct'' simpler model where the transit
depth is constant.
The variable depth likelihood is simply the product of amplitudes from the
initial search
\begin{eqnarray}
p_\mathrm{var}(\{\flux\}_n\,|\,\period,\,\phase,\,\duration) &=&
\prod_{m=1}^{M(\period,\,\phase)} L_m \quad,
\end{eqnarray}
and the constant depth model is
\begin{eqnarray}
p_\mathrm{const}(\{\flux\}_n\,|\,\period,\,\phase,\,\duration) &=&
\prod_{m=1}^{M(\period,\,\phase)}
    \frac{L_m}{\sqrt{2\,\pi\,{\delta\bar{\depth}_m}^2}}\,\exp \left(
        -\frac{[\depth - \bar{\depth}_m]^2}{2\,{\delta\bar{\depth}_m}^2}
    \right)
\end{eqnarray}
where the maximum likelihood depth is
\begin{eqnarray}
\depth &=& {\sigma_\depth}^2\,\sum_{m=1}^{M(\period,\,\phase)}
    \frac{\bar{\depth}_m}{{\delta\bar{\depth}_m}^2}
\end{eqnarray}
and the uncertainty on this depth is given by
\begin{eqnarray}
\frac{1}{{\sigma_\depth}^2} &=& \sum_{m=1}^{M(\period,\,\phase)}
    \frac{1}{{\delta\bar{\depth}_m}^2} \quad.
\end{eqnarray}

In general, the variable depth model will \emph{always} get a higher
likelihood because it is more flexible.
Therefore, to compare these two models, we use the Bayesian Information
Criterion (BIC).
The traditional definition of the BIC is
\begin{eqnarray}
-\frac{1}{2}\,\BIC_\cdot &=&
    \ln p_\cdot(\{\flux\}_n\,|\,\period,\,\phase,\,\duration)
    - \frac{J}{2} \ln N
\end{eqnarray}
where the likelihood function is maximized, $J$ is an estimate of the model
complexity and $N$ is the effective sample size.
To emphasize that $J$ and $N$ are tuning parameters of the method, we
redefine the BIC as the TIC
\begin{eqnarray}
\TIC_\cdot &=&
    \ln p_\cdot(\{\flux\}_n\,|\,\period,\,\phase,\,\duration) - \alpha
\end{eqnarray}
and we choose $\alpha$ heuristically.
For the K2 Campaign 1 dataset, we find that $\alpha \sim 1250$ leads to robust
recovery of injected signals while still being fairly insensitive to false
signals.

To limit memory consumption, in the periodic search, we actually profile (or
maximize) over \phase\ subject to the constraint that $\TIC_\mathrm{const} >
\TIC_\mathrm{var}$.


\paragraph{Machine vetting}

To generate an initial candidate list, we start by .

\paragraph{Hand vetting}

There are many instrumental or astrophysical effects that can mimic a transit
signal in time series photometry.
For the primary \kepler\ mission, the noise induced by instrumental artifacts
on transit timescales was significantly smaller than transit depths for 
all but the smallest planets.
With \KT\ this is no longer the case.
The motion of the instrument causes a drift in the pointing of approximately
$1''$ per hour, causing variations in the flat field to dominate the 
photometric signal.

As a result, artifacts in the data induced by pointing jitter can ...

If the properties of the noise in the data can be fully understood and 
modeled, such artifacts can be separated from true astrophysical events.
Our model, however, makes no assumptions about the properties of the noise
other than that it is correlated across many stars on the detector.
Therefore, our model can occassionally be fooled by instrumental artifacts
that a more sophisticated method, or the human eye, can easily separate 
from true astrophysical signals.

To this end, we (euphamism for look at by eye) each system to identify the
most promising, likely astrophysical signals.

\paragraph{Astrophysical false positives}

A major problem with any transit search is the potential confusion between
transiting planets and eclipsing binaries.
Ground-based surveys before \kepler\ often had a false-positive rate 
of 30-40\%. (because of shit?)
In the primary \kepler\ mission, this number was significantly lower:
the false positive rate was only 5-10\% \citep{morton11}.
Such improvement was enabled by the relatively small pixels, stable 
pointing, and high precision of \kepler.
In \kepler\ data, false positives can be reliably separated from 
planet candidates by looking for differences in the position of the 
centroid of light during transit events with respect to the 
out-of-transit data (e.g. citation), variations in the depth of
alternating transits (e.g. citation), and evidence for ellipsoidal
variations between transits (e.g. citation).

In \KT, we will not be able to rely on any of these techniques to
characterize systems \textit{en masse}. 
There are typically only a handful of transits, meaning differences
between ``odd'' and ``even'' transits must be large to create
a significant difference.
Searching for ellipsoidal varaitions is hindered by the short time
baseline and the increased photometric uncertainty in \KT\ data.
Searching for centroid variations is also more difficult in \KT:
to measure a centroid offset, one must know where the centroid is
supposed to be.
Predicting a centroid position is trivial with \kepler\ data due to the 
remarkably stable pointing of the instrument. 
With \KT\ this is only possible if one understands the pointing of the 
spacecraft at a specific time.
In theory, this is possible: the spacecraft is a rigid body, and by measuring
the mean motion of stars across the detector over time, the 
Euler angles can be inferred at all times.
However, properly measuring positions of stars is a difficult problem:
the centroid depends sensitively on the (unknown) flat field, which must
be simultaneously inferred.
Therefore, false positives caused by background eclipsing binaries which could
be identified as such by \kepler\ data alone will be much more difficult to 
distinguish in \KT\ data, making high-contrast adaptive optics images of 
planet candidates an essential follow-up tool.




\section{Results}



\section{Discussion}


\paragraph{Maybe this goes in results}

We include posterior distributions of planet candidate properties, available at
\dfmtodo{thisurl}. 
These distributions are specifically \emph{NOT} marginalized over stellar 
properties.
In this work, we are agnostic about fundamental properties of the host star.
In fact, the only assumptions we make are that the star targeted by the \KT\ 
team is truly the planet host, and that there is no dilution by other stars in
any aperture. 
As a result, these posterior distributions reflect the maximum possible 
uncertainty in parameters such as the planet radius which depend sensitively 
on properties of the host star.

To use these distributions to characterize the properties of specific systems,
one should weight our samples by inferred stellar properties. 
 



\acknowledgments
It is a pleasure to thank
\ldots\
for helpful contributions to the ideas and code presented here.
This project was partially supported by the NSF (grant AST-0908357), NASA
(grant NNX08AJ48G), and the Moore--Sloan Data Science Environment at NYU.
B.T.M. is supported by the National Science Foundation Graduate Research 
Fellowship under Grant No. DGE‐1144469.
This research made use of the NASA \project{Astrophysics Data System}.

\newcommand{\arxiv}[1]{\href{http://arxiv.org/abs/#1}{arXiv:#1}}
\begin{thebibliography}{}\raggedright

\bibitem[Crossfield \etal(2015)]{crossfield}
Crossfield, I.~J.~M., Petigura, E., Schlieder, J., \etal\ 2015,
\arxiv{1501.03798}

\bibitem[Lang \etal(2010)]{astrometry}
Lang, D., Hogg, D.~W., Mierle, K., Blanton, M., \& Roweis, S.\ 2010, \aj, 139,
1782

\bibitem[Pedregosa \etal(2011)]{sklearn}
Pedregosa, F., Varoquaux, G., Gramfort, A., \etal\ 2011, JMLR, 12, 2825

\bibitem[Smith \etal(2012)]{map-pdc2}
Smith,~J.~C., Stumpe,~M.~C., Van Cleve,~J.~E., \etal\ 2012,
\pasp, 124, 1000

\bibitem[Stumpe \etal(2012)]{map-pdc1}
Stumpe,~M.~C., Smith,~J.~C., Van Cleve,~J.~E., \etal\ 2012,
\pasp, 124, 985

\bibitem[Vanderburg \& Johnson(2014a)]{vanderberg-a}
Vanderburg, A., \& Johnson, J.~A.\ 2014, \pasp, 126, 948

\bibitem[Vanderburg \etal(2014b)]{vanderburg-b}
Vanderburg, A., Montet, B.~T., Johnson, J.~A., \etal\ 2014, \arxiv{1412.5674}

\end{thebibliography}

\clearpage
\appendix

\section{Mathematical model}\sectlabel{math}

Formally, this can be written for the light curve of the $k$-th star as
\begin{eqnarray}\eqlabel{linear-model}
\bvec{f}_k &=& \bvec{A}\,\bvec{w}_k + \mathrm{noise}
\end{eqnarray}
where
\begin{eqnarray}
\bvec{f}_k &=& \left (\begin{array}{cccc}
    f_{k,1} & f_{k,2} & \cdots & f_{k,N}
\end{array}\right )^\T
\end{eqnarray}
is the list aperture fluxes for star $k$ observed at $N$ times
\begin{eqnarray}
\bvec{t} &=& \left (\begin{array}{cccc}
    t_{1} & t_{2} & \cdots & t_{N}
\end{array}\right )^\T \quad.
\end{eqnarray}
In \eq{linear-model}, the design matrix is given by
\begin{eqnarray}
\bvec{A} &=& \left (\begin{array}{cccccc}
    x_{1,1} & x_{2,1} & \cdots & x_{J,1} & 1 & m_\bvec{\theta}(t_1) \\
    x_{1,2} & x_{2,2} & \cdots & x_{J,2} & 1 & m_\bvec{\theta}(t_2) \\
    && \vdots &&&\\
    x_{1,N} & x_{2,N} & \cdots & x_{J,N} & 1 & m_\bvec{\theta}(t_N)
\end{array}\right )
\end{eqnarray}
where the $x_{j,n}$ are the basis ELCs---with the index $j$ running over
components and the index $n$ running over time---and $m_\bvec{\theta}(t)$ is
the transit model
\begin{eqnarray}
m_\bvec{\theta}(t) &=& \left\{\begin{array}{cl}
-1 & \mathrm{if\,}t\,\mathrm{in\,transit} \\
0 & \mathrm{otherwise}
\end{array}\right.
\end{eqnarray}
parameterized by a period, phase, and transit duration (these parameters are
denoted by \bvec{\theta}).

Assuming that the uncertainties on $\bvec{f}_k$ are Gaussian and constant,
the maximum likelihood solution for \bvec{w} is
\begin{eqnarray}
{\bvec{w}_k}^* &\gets& \left( \bvec{A}^\T\,\bvec{A} \right)^{-1}\,
                       \bvec{A}^\T\,\bvec{f}_k
\end{eqnarray}
and the marginalized likelihood function for the transit depth is a Gaussian
with the mean given by the last element of ${\bvec{w}_k}^*$ and the variance
given by the lower-right element of the matrix
\begin{eqnarray}
{\bvec{\delta w}_k}^2 &\gets& {\sigma_k}^2 \,
            \left( \bvec{A}^\T\,\bvec{A} \right)^{-1}
\end{eqnarray}
where $\sigma_k$ is the uncertainty on $\bvec{f}_k$.
The amplitude of this Gaussian is given by
\begin{eqnarray}\eqlabel{depth-likelihood}
\mathcal{L}_k &=& \frac{1}{(2\,\pi\,{\sigma_k}^2)^{N/2}}\,\exp\left(
-\frac{1}{2\,{\sigma_k}^2}\,
\left| \bvec{f}_k - \bvec{A}\,{\bvec{w}_k}^* \right|^2
\right)
\end{eqnarray}
evaluated at the maximum likelihood value ${\bvec{w}_k}^*$.

Remember that the likelihood defined in \eq{depth-likelihood} is a function of
the parameters \bvec{\theta} (the period, phase, and duration of the planet).
In principle, this means that a transit search can proceed by building a dense
3D grid of \bvec{\theta} values and evaluating the likelihood of the data
$\mathcal{L}_k$ using \eq{depth-likelihood} at each point in the grid.

\clearpage

\begin{figure}[p]
\begin{center}
\includegraphics{figures/pca.pdf}
\end{center}
\caption{%
The top 10 eigen light curves (ELCs) generated by running principal component
analysis on all the aperture photometry from Campaign 1.
\figlabel{pca}}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics{figures/corr.pdf}
\end{center}
\caption{%
A demonstration of the ELC fit to the aperture photometry for EPIC 201374602.
\emph{Top:} The black points show the aperture photometry and the green line
is the maximum likelihood linear combination of ELCs.
The estimated 6-hour precision of the raw photometry is 264 ppm.
\emph{Bottom:} The points show the residuals of the data away from the ELC
prediction.
The 6-hour precision of this light curve is 31 ppm.
\figlabel{corr}}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics{figures/linear.pdf}
\end{center}
\caption{%
The estimated transit depth as a function of transit time as computed in a
linear search.
This light curve has an injected signal with transits at the times indicated
with green vertical lines.
\figlabel{linear}}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics{figures/periodic.pdf}
\end{center}
\caption{%
The periodogram for the same signal as in \fig{linear}.
This plot was generated by the periodic search procedure.
The true period is indicated by the vertical green line.
\figlabel{periodic}}
\end{figure}

\begin{figure}[p]
\begin{center}
\includegraphics{figures/depth_s2n.pdf}
\end{center}
\caption{%
The signal-to-noise of the transit depth as a function of period for the same
signal as in \fig{periodic}.
The correct injected period is indicated by the vertical green line.
\figlabel{depth-s2n}}
\end{figure}

\end{document}
